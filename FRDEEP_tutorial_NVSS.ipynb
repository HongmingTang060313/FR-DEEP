{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script mostly follows [the standard CIFAR10 Pytorch example](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html). It extracts extract greyscale images from the dataset.\n",
    "\n",
    "The steps are:\n",
    "\n",
    "1. Load and normalizing the FRDEEP-N training and test datasets using torchvision\n",
    "2. Define a Convolutional Neural Network\n",
    "3. Define a loss function\n",
    "4. Train the network on the training data\n",
    "5. Test the network on the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First import some standard python libraries for plotting stuff and handling arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then import the pytorch and torchvision libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then import the pytorch neural network stuff:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then import the oprimization library from pytorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally import the FRDEEP-N pytorch dataset class. This is not provided with pytorch, you need to [grab it from the FRDEEP github](\n",
    "https://github.com/HongmingTang060313/FR-DEEP/blob/master/FRDEEP.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FRDEEP import FRDEEPN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of torchvision datasets are PILImage images of range [0, 1]. We transform them to Tensors of normalized range [-1, 1]. CenterCrop() is adapted to unify image angular size between FRDEEP-F and FRDEEP-N images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.CenterCrop(18),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize([0.5],[0.5])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the training and test datasets. The first time you do this it will download the data to your working directory, but once the data is there it will just use it without repeating the download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = FRDEEPN(root='./NVSS_data', train=True, download=False, transform=transform)\n",
    "batch_size_train = 2\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size_train, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = FRDEEPN(root='./NVSS_data', train=False, download=False, transform=transform)\n",
    "batch_size_test = 2\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size_test, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two classes in this dataset: FRI and FRII:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ('FRI', 'FRII')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A little function to display images nicely:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    # unnormalize\n",
    "    img = img / 2 + 0.5\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at some randomly selected samples to see how they appear:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  FRI  FRII\n"
     ]
    }
   ],
   "source": [
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(batch_size_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a neural network that takes greyscale images as input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 3 * 3, 120) # (16 * 34 * 34,120) in the case of no image crop\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # conv1 output width: input_width - (kernel_size - 1) => 18 - (5-1) = 14\n",
    "        # pool 1 output width: int(input_width/2) => 7\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        # conv2 output width: input_width - (kernel_size - 1) => 7 - (5-1) = 3\n",
    "        # x = self.pool(F.relu(self.conv2(x))) # replace the following line with this if not cropping images.\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view(-1, 16 * 3 * 3) # (16 * 34 * 34,120) in the case of no image crop\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 6, 14, 14]             156\n",
      "         MaxPool2d-2              [-1, 6, 7, 7]               0\n",
      "            Conv2d-3             [-1, 16, 3, 3]           2,416\n",
      "            Linear-4                  [-1, 120]          17,400\n",
      "            Linear-5                   [-1, 84]          10,164\n",
      "            Linear-6                   [-1, 10]             850\n",
      "================================================================\n",
      "Total params: 30,986\n",
      "Trainable params: 30,986\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 0.12\n",
      "Estimated Total Size (MB): 0.13\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "summary(net,(1,18,18)) # (1,150,150) in the case of no image crop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use Classification Cross-Entropy loss and Adagrad with momentum for optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adagrad(net.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run 10 epochs of training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    50] loss: 0.898\n",
      "[1,   100] loss: 0.722\n",
      "[1,   150] loss: 0.667\n",
      "[1,   200] loss: 0.705\n",
      "[1,   250] loss: 0.653\n",
      "[2,    50] loss: 0.645\n",
      "[2,   100] loss: 0.534\n",
      "[2,   150] loss: 0.680\n",
      "[2,   200] loss: 0.581\n",
      "[2,   250] loss: 0.527\n",
      "[3,    50] loss: 0.512\n",
      "[3,   100] loss: 0.593\n",
      "[3,   150] loss: 0.529\n",
      "[3,   200] loss: 0.530\n",
      "[3,   250] loss: 0.526\n",
      "[4,    50] loss: 0.561\n",
      "[4,   100] loss: 0.575\n",
      "[4,   150] loss: 0.549\n",
      "[4,   200] loss: 0.492\n",
      "[4,   250] loss: 0.502\n",
      "[5,    50] loss: 0.474\n",
      "[5,   100] loss: 0.466\n",
      "[5,   150] loss: 0.594\n",
      "[5,   200] loss: 0.501\n",
      "[5,   250] loss: 0.544\n",
      "[6,    50] loss: 0.475\n",
      "[6,   100] loss: 0.514\n",
      "[6,   150] loss: 0.423\n",
      "[6,   200] loss: 0.513\n",
      "[6,   250] loss: 0.619\n",
      "[7,    50] loss: 0.504\n",
      "[7,   100] loss: 0.568\n",
      "[7,   150] loss: 0.446\n",
      "[7,   200] loss: 0.508\n",
      "[7,   250] loss: 0.511\n",
      "[8,    50] loss: 0.509\n",
      "[8,   100] loss: 0.568\n",
      "[8,   150] loss: 0.542\n",
      "[8,   200] loss: 0.517\n",
      "[8,   250] loss: 0.400\n",
      "[9,    50] loss: 0.541\n",
      "[9,   100] loss: 0.473\n",
      "[9,   150] loss: 0.458\n",
      "[9,   200] loss: 0.553\n",
      "[9,   250] loss: 0.461\n",
      "[10,    50] loss: 0.523\n",
      "[10,   100] loss: 0.547\n",
      "[10,   150] loss: 0.456\n",
      "[10,   200] loss: 0.508\n",
      "[10,   250] loss: 0.491\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "nepoch = 10  # number of epochs\n",
    "print_num = 50\n",
    "for epoch in range(nepoch):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % print_num == (print_num-1):    # print every 50 mini-batches.\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / print_num))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll try out a couple of test samples just for visual kicks. First load them up and take a look at the true labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GroundTruth:    FRI  FRII\n"
     ]
    }
   ],
   "source": [
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(batch_size_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then see what the network predicts that they are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = net(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:   FRII  FRII\n"
     ]
    }
   ],
   "source": [
    "_, predicted = torch.max(outputs, 1)\n",
    "print('Predicted: ', ' '.join('%5s' % classes[predicted[j]] for j in range(batch_size_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now calculate the overall accuracy of the network on **all** the test images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 50 test images: 72 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 50 test images: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a imbalanced dataset, so let's take a look at the accuracy for individual classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(batch_size_test):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of   FRI : 40 %\n",
      "Accuracy of  FRII : 96 %\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(classes)):\n",
    "    print('Accuracy of %5s : %2d %%' % (classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
