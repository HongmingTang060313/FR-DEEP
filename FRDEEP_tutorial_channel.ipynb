{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script mostly follows [the standard CIFAR10 Pytorch example](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html). It extracts a single \"channel\" from the dataset ï¼ˆif image inputs are in RGB and treats it as a greyscale image or directly extract greyscale image.\n",
    "\n",
    "The steps are:\n",
    "\n",
    "1. Load and normalizing the FRDEEP-N/FRDEEP-F training and test datasets using torchvision\n",
    "2. Define a Convolutional Neural Network\n",
    "3. Define a loss function\n",
    "4. Train the network on the training data\n",
    "5. Test the network on the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First import some standard python libraries for plotting stuff and handling arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then import the pytorch and torchvision libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then import the pytorch neural network stuff:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then import the oprimization library from pytorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally import the FRDEEP pytorch dataset class. This is not provided with pytorch, you need to [grab it from the FRDEEP github](\n",
    "https://github.com/HongmingTang060313/FR-DEEP/blob/master/htru3.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FRDEEP import FRDEEPN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a torchvision transform to extract a single channel from the multi-channel dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_channel(x,c,color=None):\n",
    "    \n",
    "    from PIL import Image\n",
    "    \n",
    "    np_img = np.array(x, dtype=np.uint8)\n",
    "    if color=='RGB':\n",
    "        ch_img = np_img[:,:,c]\n",
    "    elif color=='grey':\n",
    "        ch_img = np_img\n",
    "    img = Image.fromarray(ch_img,'L')\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of torchvision datasets are PILImage images of range [0, 1]. We transform them to Tensors of normalized range [-1, 1]. The first transform extracts Channel 0 (DM surface) from the data.\n",
    "\n",
    "Input images can be cropped if needed using the crop(img,x,y,w,h) function, this function can be disable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.Lambda(lambda x: select_channel(x,0,'grey')), # 'RGB' in the context of htru1\n",
    "     transforms.Lambda(lambda x: transforms.functional.crop(x,66, 66, 18, 18)), # crop image [20190423 22:34]\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize([0.5],[0.5])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the training and test datasets. The first time you do this it will download the data to your working directory, but once the data is there it will just use it without repeating the download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(550, 150, 150, 1)\n"
     ]
    }
   ],
   "source": [
    "trainset = FRDEEPN(root='./NVSS_data', train=True, download=False, transform=transform) # root='./data' in the context of htru1 \n",
    "batch_size_train = 2\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size_train, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 150, 150, 1)\n"
     ]
    }
   ],
   "source": [
    "testset = FRDEEPN(root='./NVSS_data', train=False, download=False, transform=transform) # root='./data' in the context of htru1 \n",
    "batch_size_test = 2\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size_test, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two classes in this dataset: FRI and FRII:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ('FRI', 'FRII')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A little function to display images nicely:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    # unnormalize\n",
    "    img = img / 2 + 0.5\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at some randomly selected samples to see how they appear:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 18, 18])\n"
     ]
    }
   ],
   "source": [
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "print(np.shape(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  FRI  FRII\n"
     ]
    }
   ],
   "source": [
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(batch_size_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a neural network that takes 1-channel images as input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 3 * 3, 120) # (16 * 5 * 5,120) in the case of pulsars  (16 * 3 * 3) for NVSS [20190423 22:34]\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # conv1 output width: input_width - (kernel_size - 1) => 18 - (5-1) = 14\n",
    "        # pool 1 output width: int(input_width/2) => 7\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        # conv2 output width: input_width - (kernel_size - 1) => 7 - (5-1) = 3\n",
    "        # too small to pool\n",
    "        # x = self.pool(F.relu(self.conv2(x)))  cancel pool 2 for NVSS [20190423 22:34]\n",
    "        x = F.relu(self.conv2(x)) # cancel pool 2 for NVSS [20190423 22:34]\n",
    "        x = x.view(-1, 16 * 3 * 3) # (-1,16 * 5 * 5) in the case of pulsars (16 * 3 * 3) for NVSS [20190423 22:34]\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 6, 14, 14]             156\n",
      "         MaxPool2d-2              [-1, 6, 7, 7]               0\n",
      "            Conv2d-3             [-1, 16, 3, 3]           2,416\n",
      "            Linear-4                  [-1, 120]          17,400\n",
      "            Linear-5                   [-1, 84]          10,164\n",
      "            Linear-6                   [-1, 10]             850\n",
      "================================================================\n",
      "Total params: 30,986\n",
      "Trainable params: 30,986\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 0.12\n",
      "Estimated Total Size (MB): 0.13\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "summary(net,(1,18,18)) # (1,32,32) for htrl1 (1,18,18) for NVSS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use Classification Cross-Entropy loss and Adagrad with momentum for optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adagrad(net.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a few epochs of training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    50] loss: 0.883\n",
      "[1,   100] loss: 0.707\n",
      "[1,   150] loss: 0.670\n",
      "[1,   200] loss: 0.681\n",
      "[1,   250] loss: 0.685\n",
      "[2,    50] loss: 0.628\n",
      "[2,   100] loss: 0.490\n",
      "[2,   150] loss: 0.634\n",
      "[2,   200] loss: 0.510\n",
      "[2,   250] loss: 0.598\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "nepoch = 2  # number of epochs\n",
    "print_num = 50\n",
    "for epoch in range(nepoch):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % print_num == (print_num-1):    # print every 50 mini-batches [2000 in the context of htru1]\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / print_num))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll try out a couple of test samples just for visual kicks. First load them up and take a look at the true labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GroundTruth:    FRI  FRII\n"
     ]
    }
   ],
   "source": [
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(batch_size_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then see what the network predicts that they are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = net(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:    FRI  FRII\n"
     ]
    }
   ],
   "source": [
    "_, predicted = torch.max(outputs, 1)\n",
    "print('Predicted: ', ' '.join('%5s' % classes[predicted[j]] for j in range(batch_size_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now calculate the overall accuracy of the network on **all** the test images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 50 test images: 76 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 50 test images: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a seriously imbalanced dataset, so let's take a look at the accuracy for individual classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(batch_size_test):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of   FRI : 50 %\n",
      "Accuracy of  FRII : 96 %\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(classes)):\n",
    "    print('Accuracy of %5s : %2d %%' % (classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
