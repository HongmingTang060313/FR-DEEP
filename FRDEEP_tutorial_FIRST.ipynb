{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script mostly follows [the standard CIFAR10 Pytorch example](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html). It extracts grey scale images from the dataset.\n",
    "\n",
    "The steps are:\n",
    "\n",
    "1. Load and normalizing the FRDEEP-F training and test datasets using torchvision\n",
    "2. Define a Convolutional Neural Network\n",
    "3. Define a loss function\n",
    "4. Train the network on the training data\n",
    "5. Test the network on the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First import some standard python libraries for plotting stuff and handling arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then import the pytorch, torchvision and torchsummary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then import the pytorch neural network stuff:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then import the oprimization library from pytorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally import the FRDEEP-F pytorch dataset class. This is not provided with pytorch, you need to [grab it from the FRDEEP github](\n",
    "https://github.com/HongmingTang060313/FR-DEEP/blob/master/FRDEEP.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FRDEEP import FRDEEPF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of torchvision datasets are PILImage images of range [0, 1]. We transform them to Tensors of normalized range [-1, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize([0.5],[0.5])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the training and test datasets. The first time you do this it will download the data to your working directory, but once the data is there it will just use it without repeating the download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainset = FRDEEPF(root='./FIRST_data', train=True, download=True, transform=transform)  \n",
    "batch_size_train = 2\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size_train, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "testset = FRDEEPF(root='./FIRST_data', train=False, download=True, transform=transform) \n",
    "batch_size_test = 2\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size_test, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two classes in this dataset: FRI and FRII:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ('FRI', 'FRII')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A little function to display images nicely:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    # unnormalize\n",
    "    img = img / 2 + 0.5\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at some randomly selected samples to see how they appear:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  FRI  FRII\n"
     ]
    }
   ],
   "source": [
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(batch_size_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a neural network that takes greyscale images as input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 34 * 34, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # conv1 output width: input_width - (kernel_size - 1) => 150 - (5-1) = 146\n",
    "        # pool 1 output width: int(input_width/2) => 73\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        # conv2 output width: input_width - (kernel_size - 1) => 73 - (5-1) = 69\n",
    "        # pool 2 output width: int(input_width/2) => 34\n",
    "        x = self.pool(F.relu(self.conv2(x)))  \n",
    "        x = x.view(-1, 16 * 34 * 34)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 6, 146, 146]             156\n",
      "         MaxPool2d-2            [-1, 6, 73, 73]               0\n",
      "            Conv2d-3           [-1, 16, 69, 69]           2,416\n",
      "         MaxPool2d-4           [-1, 16, 34, 34]               0\n",
      "            Linear-5                  [-1, 120]       2,219,640\n",
      "            Linear-6                   [-1, 84]          10,164\n",
      "            Linear-7                   [-1, 10]             850\n",
      "================================================================\n",
      "Total params: 2,233,226\n",
      "Trainable params: 2,233,226\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.09\n",
      "Forward/backward pass size (MB): 1.94\n",
      "Params size (MB): 8.52\n",
      "Estimated Total Size (MB): 10.55\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "summary(net,(1,150,150))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use Classification Cross-Entropy loss and Adagrad with momentum for optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adagrad(net.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run 10 epochs of training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    50] loss: 1.272\n",
      "[1,   100] loss: 0.750\n",
      "[1,   150] loss: 0.733\n",
      "[1,   200] loss: 0.723\n",
      "[1,   250] loss: 0.696\n",
      "[2,    50] loss: 0.652\n",
      "[2,   100] loss: 0.633\n",
      "[2,   150] loss: 0.670\n",
      "[2,   200] loss: 0.623\n",
      "[2,   250] loss: 0.604\n",
      "[3,    50] loss: 0.569\n",
      "[3,   100] loss: 0.584\n",
      "[3,   150] loss: 0.493\n",
      "[3,   200] loss: 0.547\n",
      "[3,   250] loss: 0.549\n",
      "[4,    50] loss: 0.522\n",
      "[4,   100] loss: 0.449\n",
      "[4,   150] loss: 0.494\n",
      "[4,   200] loss: 0.470\n",
      "[4,   250] loss: 0.536\n",
      "[5,    50] loss: 0.435\n",
      "[5,   100] loss: 0.463\n",
      "[5,   150] loss: 0.430\n",
      "[5,   200] loss: 0.458\n",
      "[5,   250] loss: 0.387\n",
      "[6,    50] loss: 0.438\n",
      "[6,   100] loss: 0.355\n",
      "[6,   150] loss: 0.411\n",
      "[6,   200] loss: 0.514\n",
      "[6,   250] loss: 0.353\n",
      "[7,    50] loss: 0.323\n",
      "[7,   100] loss: 0.381\n",
      "[7,   150] loss: 0.315\n",
      "[7,   200] loss: 0.276\n",
      "[7,   250] loss: 0.413\n",
      "[8,    50] loss: 0.313\n",
      "[8,   100] loss: 0.230\n",
      "[8,   150] loss: 0.372\n",
      "[8,   200] loss: 0.422\n",
      "[8,   250] loss: 0.268\n",
      "[9,    50] loss: 0.290\n",
      "[9,   100] loss: 0.379\n",
      "[9,   150] loss: 0.286\n",
      "[9,   200] loss: 0.257\n",
      "[9,   250] loss: 0.244\n",
      "[10,    50] loss: 0.189\n",
      "[10,   100] loss: 0.288\n",
      "[10,   150] loss: 0.295\n",
      "[10,   200] loss: 0.248\n",
      "[10,   250] loss: 0.267\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "nepoch = 10  # number of epochs\n",
    "print_num = 50\n",
    "for epoch in range(nepoch):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % print_num == (print_num-1):    # print every 50 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / print_num))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll try out a couple of test samples just for visual kicks. First load them up and take a look at the true labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GroundTruth:   FRII  FRII\n"
     ]
    }
   ],
   "source": [
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(batch_size_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then see what the network predicts that they are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = net(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:    FRI   FRI\n"
     ]
    }
   ],
   "source": [
    "_, predicted = torch.max(outputs, 1)\n",
    "print('Predicted: ', ' '.join('%5s' % classes[predicted[j]] for j in range(batch_size_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now calculate the overall accuracy of the network on **all** the test images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 50 test images: 74 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 50 test images: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a imbalanced dataset, so let's take a look at the accuracy for individual classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(batch_size_test):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of   FRI : 81 %\n",
      "Accuracy of  FRII : 67 %\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(classes)):\n",
    "    print('Accuracy of %5s : %2d %%' % (classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
