{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script mostly follows [the standard CIFAR10 Pytorch example](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html). It extracts grey scale images from the dataset.\n",
    "\n",
    "The steps are:\n",
    "\n",
    "1. Load and normalizing the FRDEEP-F training and test datasets using torchvision\n",
    "2. Define a Convolutional Neural Network\n",
    "3. Define a loss function\n",
    "4. Train the network on the training data\n",
    "5. Test the network on the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First import some standard python libraries for plotting stuff and handling arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then import the pytorch and torchvision libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then import the pytorch neural network stuff:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then import the oprimization library from pytorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally import the FRDEEP-F pytorch dataset class. This is not provided with pytorch, you need to [grab it from the FRDEEP github](\n",
    "https://github.com/HongmingTang060313/FR-DEEP/blob/master/htru3.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FRDEEP import FRDEEPF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of torchvision datasets are PILImage images of range [0, 1]. We transform them to Tensors of normalized range [-1, 1]. The first transform extracts Channel 0 (DM surface) from the data.\n",
    "\n",
    "Input images can be cropped if needed using the crop(img,x,y,w,h) function, this function can be disable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize([0.5],[0.5])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the training and test datasets. The first time you do this it will download the data to your working directory, but once the data is there it will just use it without repeating the download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainset = FRDEEPF(root='./FIRST_data', train=True, download=True, transform=transform)  \n",
    "batch_size_train = 2\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size_train, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "testset = FRDEEPF(root='./FIRST_data', train=False, download=True, transform=transform) \n",
    "batch_size_test = 2\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size_test, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two classes in this dataset: FRI and FRII:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ('FRI', 'FRII')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A little function to display images nicely:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    # unnormalize\n",
    "    img = img / 2 + 0.5\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at some randomly selected samples to see how they appear:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " FRII   FRI\n"
     ]
    }
   ],
   "source": [
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(batch_size_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a neural network that takes 1-channel images as input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 34 * 34, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # conv1 output width: input_width - (kernel_size - 1) => 150 - (5-1) = 146\n",
    "        # pool 1 output width: int(input_width/2) => 73\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        # conv2 output width: input_width - (kernel_size - 1) => 73 - (5-1) = 69\n",
    "        # pool 2 output width: int(input_width/2) => 34\n",
    "        x = self.pool(F.relu(self.conv2(x)))  \n",
    "        x = x.view(-1, 16 * 34 * 34)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 6, 146, 146]             156\n",
      "         MaxPool2d-2            [-1, 6, 73, 73]               0\n",
      "            Conv2d-3           [-1, 16, 69, 69]           2,416\n",
      "         MaxPool2d-4           [-1, 16, 34, 34]               0\n",
      "            Linear-5                  [-1, 120]       2,219,640\n",
      "            Linear-6                   [-1, 84]          10,164\n",
      "            Linear-7                   [-1, 10]             850\n",
      "================================================================\n",
      "Total params: 2,233,226\n",
      "Trainable params: 2,233,226\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.09\n",
      "Forward/backward pass size (MB): 1.94\n",
      "Params size (MB): 8.52\n",
      "Estimated Total Size (MB): 10.55\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "summary(net,(1,150,150))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use Classification Cross-Entropy loss and Adagrad with momentum for optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adagrad(net.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a few epochs of training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    50] loss: 3.409\n",
      "[1,   100] loss: 0.881\n",
      "[1,   150] loss: 0.710\n",
      "[1,   200] loss: 0.661\n",
      "[1,   250] loss: 0.695\n",
      "[2,    50] loss: 0.730\n",
      "[2,   100] loss: 0.695\n",
      "[2,   150] loss: 0.700\n",
      "[2,   200] loss: 0.696\n",
      "[2,   250] loss: 0.641\n",
      "[3,    50] loss: 0.686\n",
      "[3,   100] loss: 0.690\n",
      "[3,   150] loss: 0.645\n",
      "[3,   200] loss: 0.620\n",
      "[3,   250] loss: 0.616\n",
      "[4,    50] loss: 0.538\n",
      "[4,   100] loss: 0.566\n",
      "[4,   150] loss: 0.596\n",
      "[4,   200] loss: 0.629\n",
      "[4,   250] loss: 0.486\n",
      "[5,    50] loss: 0.534\n",
      "[5,   100] loss: 0.518\n",
      "[5,   150] loss: 0.533\n",
      "[5,   200] loss: 0.476\n",
      "[5,   250] loss: 0.434\n",
      "[6,    50] loss: 0.446\n",
      "[6,   100] loss: 0.409\n",
      "[6,   150] loss: 0.442\n",
      "[6,   200] loss: 0.435\n"
     ]
    }
   ],
   "source": [
    "nepoch = 10  # number of epochs\n",
    "print_num = 50\n",
    "for epoch in range(nepoch):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % print_num == (print_num-1):    # print every 50 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / print_num))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll try out a couple of test samples just for visual kicks. First load them up and take a look at the true labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(batch_size_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then see what the network predicts that they are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = net(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, predicted = torch.max(outputs, 1)\n",
    "print('Predicted: ', ' '.join('%5s' % classes[predicted[j]] for j in range(batch_size_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now calculate the overall accuracy of the network on **all** the test images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 50 test images: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a seriously imbalanced dataset, so let's take a look at the accuracy for individual classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(batch_size_test):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(classes)):\n",
    "    print('Accuracy of %5s : %2d %%' % (classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
